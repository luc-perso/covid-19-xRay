{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aomJUY_gvXed",
   "metadata": {
    "id": "aomJUY_gvXed"
   },
   "source": [
    "Initialise code for google colab\n",
    "\n",
    "Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GAjcLhjCgpxv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4450,
     "status": "ok",
     "timestamp": 1671446275823,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "GAjcLhjCgpxv",
    "outputId": "e34c2966-478f-439f-c4bb-ea2036820438"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_addons as tfa\n",
    "print(tfa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.11\n",
    "!pip install tensorflow-addons==0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J3iW3Uff2Jyh",
   "metadata": {
    "id": "J3iW3Uff2Jyh"
   },
   "source": [
    "Create data base files under google colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D1ubGnuD2CXZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88494,
     "status": "ok",
     "timestamp": 1671446369482,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "D1ubGnuD2CXZ",
    "outputId": "79fec16a-f91c-413d-f571-dd30d3435e5f"
   },
   "outputs": [],
   "source": [
    "!unzip -q '/content/drive/MyDrive/data_equalize.zip' -d '/content/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sMjhaRhC-HQM",
   "metadata": {
    "id": "sMjhaRhC-HQM"
   },
   "source": [
    "For Luc because my archive made on mac create a __MACOSX folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iAyxXHqB935f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2940,
     "status": "ok",
     "timestamp": 1671446392319,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "iAyxXHqB935f"
   },
   "outputs": [],
   "source": [
    "%rm -rf /content/__MACOSX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "nnQuhv17x7hK",
   "metadata": {
    "id": "nnQuhv17x7hK"
   },
   "source": [
    "Define working directory to our jupyter repertory:\n",
    "* because path to the different repertories (./data, ./output...) are define relatevly to jupyter one\n",
    "* let import _mypath which add ./lib to python path in order to import our own define libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9WqQG1rjjHGq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1671446474973,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "9WqQG1rjjHGq",
    "outputId": "89a23162-4e46-4cd1-f96d-5c67bb7996aa"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/covid-19-xRay/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c135d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _mypath\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.applications import DenseNet201\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.path_origin_data import build_data_paths \n",
    "from database.path_origin_data import lung_name, infection_name\n",
    "from database.path_origin_data import train_name, test_name, valid_name\n",
    "from database.path_origin_data import normal_name, covid_name, no_covid_name\n",
    "from database.path_origin_data import images_name, lung_mask_name, infection_mask_name\n",
    "\n",
    "from database.dataset import build_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76855cab",
   "metadata": {
    "id": "76855cab"
   },
   "source": [
    "Build paths and variables for reading data base hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "X9Ngq0q01zxH",
   "metadata": {
    "id": "X9Ngq0q01zxH"
   },
   "outputs": [],
   "source": [
    "# for local use\n",
    "db_path = '../data_equalize'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KsCYZqOl137k",
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1671446525765,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "KsCYZqOl137k"
   },
   "outputs": [],
   "source": [
    "# for google colab use\n",
    "db_path = '/content/data_equalize'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join('..', 'output')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec096c7",
   "metadata": {
    "id": "fec096c7"
   },
   "source": [
    "Structure to manage paths in data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1271f47",
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1671446527378,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "a1271f47"
   },
   "outputs": [],
   "source": [
    "data_paths = build_data_paths()\n",
    "idx = pd.IndexSlice\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6849 files belonging to 1 classes.\n",
      "Found 7658 files belonging to 1 classes.\n",
      "Found 7208 files belonging to 1 classes.\n",
      "Found 2140 files belonging to 1 classes.\n",
      "Found 2394 files belonging to 1 classes.\n",
      "Found 2253 files belonging to 1 classes.\n",
      "Found 1712 files belonging to 1 classes.\n",
      "Found 1902 files belonging to 1 classes.\n",
      "Found 1802 files belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=(TensorSpec(shape=(256, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = data_paths['path']\n",
    "\n",
    "ds_train = build_dataset(db_path, paths, db=[lung_name], ds=[train_name])\n",
    "ds_test = build_dataset(db_path, paths, db=[lung_name], ds=[test_name])\n",
    "ds_valid = build_dataset(db_path, paths, db=[lung_name], ds=[valid_name])\n",
    "ds_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeat_chan(img, label):\n",
    "  return tf.repeat(img, 3, axis=-1), label\n",
    "\n",
    "ds_train = ds_train.map(repeat_chan)\n",
    "ds_test = ds_test.map(repeat_chan)\n",
    "ds_valid = ds_valid.map(repeat_chan)\n",
    "ds_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "image_size = 256\n",
    "input_shape = (image_size, image_size, 3)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# data augmentation\n",
    "flip = \"horizontal\"\n",
    "rotation_factor = 0.1\n",
    "zoom_height_factor = 0.2\n",
    "zoom_width_factor = 0.2\n",
    "\n",
    "mlp_head_units = [128, 64]  # Size of the dense layers of the final classifier\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[123.78828 123.78828 123.78828]]]], shape=(1, 1, 1, 3), dtype=float32)\n",
      "tf.Tensor([[[[5787.6235 5787.6235 5787.6235]]]], shape=(1, 1, 1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "norm_layer = layers.Normalization()\n",
    "\n",
    "ds_norm = ds_train.map(lambda img, l: img).take(1000).batch(batch_size)\n",
    "norm_layer.adapt(ds_norm)\n",
    "print(norm_layer.mean)\n",
    "print(norm_layer.variance)\n",
    "\n",
    "data_augmentation_seq = keras.Sequential(\n",
    "    [\n",
    "      norm_layer,\n",
    "      # layers.Resizing(image_size, image_size),\n",
    "      # layers.RandomFlip(flip),\n",
    "      # layers.RandomRotation(rotation_factor),\n",
    "      # layers.RandomZoom(height_factor=zoom_height_factor, width_factor=zoom_width_factor),\n",
    "    ],\n",
    "    name='data_augmentation'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myLayers.mlp import mlp\n",
    "\n",
    "# preproc = tf.keras.applications.vgg16.preprocess_input(augmented)\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "# preproc = tf.keras.applications.vgg19.preprocess_input(input)\n",
    "# baseModel = VGG19(weights=\"imagenet\", include_top=False,input_tensor=layers.Input(shape=(image_size, image_size, 3)))\n",
    "# preproc = tf.keras.applications.DenseNet201.preprocess_input(input)\n",
    "# baseModel = DenseNet201(weights=\"imagenet\", include_top=False,input_tensor=layers.Input(shape=(image_size, image_size, 3)))\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "augmented = data_augmentation_seq(inputs)\n",
    "base = baseModel(augmented)\n",
    "# construct the head of the model that will be placed on top of the\n",
    "head = layers.AveragePooling2D(pool_size=(8, 8))(base)\n",
    "head = layers.Flatten(name=\"flatten\")(head)\n",
    "features = mlp(head, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "# Classify outputs.\n",
    "softmax = layers.Dense(3, activation='softmax', kernel_initializer='random_normal')(features)\n",
    "\n",
    "# Create the Keras model.\n",
    "model = keras.Model(inputs=inputs, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, None, None, 3)    7         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 1, 1, 512)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,788,810\n",
      "Trainable params: 74,115\n",
      "Non-trainable params: 14,714,695\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 18:14:14.483397: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 6895 of 21715\n",
      "2022-12-21 18:14:24.480651: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 14154 of 21715\n",
      "2022-12-21 18:14:34.481670: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 19560 of 21715\n",
      "2022-12-21 18:14:37.934767: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 258/5429 [>.............................] - ETA: 1:47:24 - loss: 1.0340 - accuracy: 0.4748"
     ]
    }
   ],
   "source": [
    "from run_exp.standard import run_experiment\n",
    "\n",
    "checkpoint_filepath = os.path.join(output_path, \"checkpoint_transfer\")\n",
    "run_experiment(model,\n",
    "              ds_train, ds_test, ds_valid,\n",
    "              batch_size=batch_size, num_epochs=num_epochs,\n",
    "              learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "              checkpoint_filepath=checkpoint_filepath,\n",
    "              from_logits=False, label_smoothing=label_smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid-19-xRay-gI8RPtYc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec449b28ee1275c8ed3472cdab9bc054b62d41bc2731e9c066fdcbfc125fb022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
