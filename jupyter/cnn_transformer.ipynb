{"cells":[{"cell_type":"markdown","metadata":{"id":"aomJUY_gvXed"},"source":["Initialise code for google colab\n","\n","Mount google drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19142,"status":"ok","timestamp":1675005956808,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"GAjcLhjCgpxv","outputId":"87bf36dd-2f11-40c7-ec58-a5a12db6fb88"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2558,"status":"ok","timestamp":1675005935258,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"A4muRHlqelbj","outputId":"164b6f74-de75-42c0-e946-279e82e32183"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.11.0\n","0.19.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","import tensorflow_addons as tfa\n","print(tfa.__version__)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61908,"status":"ok","timestamp":1675005913429,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"dV7s-SFUelbk","outputId":"70f59214-9c55-4384-be67-7385f0c5e640"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.11\n","  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (21.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.15.0)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.3.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.51.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (4.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (2.2.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (0.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.14.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (15.0.6.1)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (1.6.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (3.3.0)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (0.29.0)\n","Collecting flatbuffers>=2.0\n","  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (3.1.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11) (57.4.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11) (0.38.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.16.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.25.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (0.4.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.11) (3.0.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (5.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11) (6.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (4.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.2.2)\n","Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","Successfully installed flatbuffers-23.1.21 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons==0.19\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons==0.19) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons==0.19) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons==0.19) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.19.0\n"]}],"source":["!pip install tensorflow==2.11\n","!pip install tensorflow-addons==0.19"]},{"cell_type":"markdown","metadata":{"id":"J3iW3Uff2Jyh"},"source":["Create data base files under google colab environment"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":26350,"status":"ok","timestamp":1675005995952,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"D1ubGnuD2CXZ"},"outputs":[],"source":["!unzip -q '/content/drive/MyDrive/data_equalize.zip' -d '/content/'"]},{"cell_type":"markdown","metadata":{"id":"nnQuhv17x7hK"},"source":["Define working directory to our jupyter repertory:\n","* because path to the different repertories (./data, ./output...) are define relatevly to jupyter one\n","* let import _mypath which add ./lib to python path in order to import our own define libraries\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1225,"status":"ok","timestamp":1675006058913,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"B2HJyd5welbm","outputId":"2c3aac34-7d36-4c6b-a8a5-781a89547d91"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/covid-19-xRay/jupyter\n"]}],"source":["# for google colab use\n","%cd /content/drive/MyDrive/covid-19-xRay/jupyter\n","from google.colab.patches import cv2_imshow\n","db_work_dir = '/content'\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"10je8Bxoelbn"},"outputs":[],"source":["# for local use\n","db_work_dir = '..'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1683,"status":"ok","timestamp":1675006063896,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"6c135d84"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-30 09:33:38.307799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import _mypath\n","import os\n","import shutil\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import cv2\n","from PIL import Image\n","import joblib\n","\n","%load_ext autoreload\n","%autoreload 1"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3548,"status":"ok","timestamp":1675006069297,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"YP-ouMbVelbn"},"outputs":[],"source":["%aimport database.path_origin_data\n","%aimport database.dataset\n","\n","from database.path_origin_data import build_data_paths \n","from database.path_origin_data import lung_name, infection_name\n","from database.path_origin_data import train_name, test_name, valid_name\n","from database.path_origin_data import normal_name, covid_name, no_covid_name\n","from database.path_origin_data import images_name, lung_mask_name, infection_mask_name\n","\n","from database.dataset import build_dataset\n","\n","%aimport run_exp.test\n","%aimport run_exp.classif_autoencoder\n","%aimport run_exp.standard\n","\n","from run_exp.test import compile_test_model, test_accuracy, test_conf_mat\n","from run_exp.classif_autoencoder import run_experiment as run_experiment_autoencoder\n","from run_exp.standard import run_experiment as run_experiment_pure_cnn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"76855cab"},"source":["Build paths and variables for reading data base hierarchy"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"XZhnTtABelbo"},"outputs":[],"source":["# input\n","db_name = 'data_equalize'\n","db_path = os.path.join(db_work_dir, db_name)\n","\n","# output\n","output_path = os.path.join('..', 'output', 'cnn_transformer')\n","if not os.path.exists(output_path):\n","    os.makedirs(output_path, exist_ok=True)\n","\n","log_path = os.path.join(output_path, 'log')\n","if not os.path.exists(log_path):\n","    os.makedirs(log_path, exist_ok=True)\n","\n","ckpt_path = os.path.join(output_path, 'ckpt')\n","if not os.path.exists(ckpt_path):\n","    os.makedirs(ckpt_path, exist_ok=True)\n","\n","metric_path = os.path.join(output_path, 'metric')\n","if not os.path.exists(metric_path):\n","    os.makedirs(metric_path, exist_ok=True)\n","\n","\n","activation_path = os.path.join(output_path, 'activation')\n","if not os.path.exists(activation_path):\n","    os.makedirs(activation_path, exist_ok=True)\n","\n","pattern_path = os.path.join(output_path, 'pattern')\n","if not os.path.exists(pattern_path):\n","    os.makedirs(pattern_path, exist_ok=True)\n","\n","occultation_path = os.path.join(output_path, 'occulatation')\n","if not os.path.exists(occultation_path):\n","    os.makedirs(occultation_path, exist_ok=True)\n","\n","grad_cam_path = os.path.join(output_path, 'grad_cam')\n","if not os.path.exists(grad_cam_path):\n","    os.makedirs(grad_cam_path, exist_ok=True)\n"]},{"cell_type":"markdown","metadata":{"id":"fec096c7"},"source":["Structure to manage paths in data base"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1675006081199,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"a1271f47"},"outputs":[],"source":["data_paths = build_data_paths()\n","idx = pd.IndexSlice"]},{"cell_type":"markdown","metadata":{"id":"IXCOdbU7elbp"},"source":["Create tf Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3735,"status":"ok","timestamp":1675006087575,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"FcSf_2y4elbp","outputId":"262975f4-0052-435e-93a1-8b4e5ec531c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6849 files belonging to 1 classes.\n","Found 7658 files belonging to 1 classes.\n","Found 7208 files belonging to 1 classes.\n","Found 2140 files belonging to 1 classes.\n","Found 2394 files belonging to 1 classes.\n","Found 2253 files belonging to 1 classes.\n","Found 1712 files belonging to 1 classes.\n","Found 1902 files belonging to 1 classes.\n","Found 1802 files belonging to 1 classes.\n","21715\n","21715\n"]}],"source":["\n","paths = data_paths['path']\n","\n","ds_train, ds_train_file_paths = build_dataset(db_path, paths, db=[lung_name], ds=[train_name])\n","ds_test, ds_test_file_paths = build_dataset(db_path, paths, db=[lung_name], ds=[test_name])\n","ds_valid, ds_valid_file_paths = build_dataset(db_path, paths, db=[lung_name], ds=[valid_name])\n","print(ds_train.cardinality().numpy())\n","print(len(ds_train_file_paths))\n"]},{"cell_type":"markdown","metadata":{"id":"oULvMsKSelbp"},"source":["Model parameters"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1675006094735,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"St2GGM-zelbp"},"outputs":[],"source":["batch_size_cnn = 32\n","batch_size_transformer = 32\n","\n","image_size = 256\n","input_shape = (image_size, image_size, 1)\n","\n","learning_rate = 0.001\n","weight_decay = 0.0001\n","num_epochs = 100\n","\n","label_smoothing = 0.1\n","lam_recon = 10.\n","patience = 5\n","min_delta = 0.005\n","min_delta_fine_tuning = 0.0005\n","\n","# data augmentation\n","scale = 1. / 255.\n","flip = \"horizontal\"\n","rotation_factor = 10. / 360.\n","zoom_height_factor = 0.2\n","zoom_width_factor = 0.2\n","\n","# vit\n","patch_size = 1\n","transformer_layers = 4\n","num_heads = 8\n","projection_dim = 64\n","transformer_units_rate = [2, 1]\n","mlp_head_units = [1024, 256]  # Size of the dense layers of the final classifier\n"]},{"cell_type":"markdown","metadata":{"id":"HpGXjB_lelbq"},"source":["Data augmentation"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":245,"status":"ok","timestamp":1675006099424,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"HMe_zlMGelbq"},"outputs":[],"source":["augmentation = keras.Sequential(\n","    [\n","      layers.Rescaling(scale=scale),\n","      layers.RandomFlip(flip),\n","      layers.RandomRotation(rotation_factor),\n","      layers.RandomZoom(height_factor=zoom_height_factor, width_factor=zoom_width_factor),\n","    ],\n","    name='augmentation'\n",")"]},{"cell_type":"markdown","metadata":{"id":"tQeJmFhXelbq"},"source":["cnn encoder"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1675006102452,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"LOHNNFhrelbq"},"outputs":[],"source":["from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dropout\n","\n","encoder = keras.Sequential(\n","    [\n","      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n","      layers.MaxPooling2D(pool_size = (2, 2)),\n","      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n","      layers.MaxPooling2D(pool_size = (2, 2)),\n","      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n","      layers.MaxPooling2D(pool_size = (2, 2)),\n","      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n","      layers.MaxPooling2D(pool_size = (2, 2)),\n","      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n","      layers.MaxPooling2D(pool_size = (2, 2)),\n","      layers.Flatten(),\n","    ],\n","    name='encoder'\n",")"]},{"cell_type":"markdown","metadata":{"id":"v2RYqtjZelbr"},"source":["decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ppe7MFktelbr"},"outputs":[],"source":["decoder = keras.Sequential(\n","    [\n","      layers.Dense(1024, activation='relu'),\n","      layers.Dense(256, activation='relu'),\n","      layers.Dense(np.prod(input_shape), activation='sigmoid'),\n","      layers.Reshape(target_shape=input_shape),\n","    ],\n","    name='decoder'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVIjXbtcelbr"},"outputs":[],"source":["classifier = keras.Sequential(\n","    [\n","      layers.Dense(1024, activation='relu'),\n","      layers.Dense(256, activation='relu'),\n","      layers.Dense(3, activation='softmax'),\n","    ],\n","    name='classifier'\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2692,"status":"ok","timestamp":1674808718448,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"C5XJwb7Zelbr","outputId":"4dd1b9f6-d7d8-47d5-fed4-c5f1bde2cffd"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"classif_decoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," augmentation (Sequential)   (None, 256, 256, 1)       0         \n","                                                                 \n"," encoder (Sequential)        (None, 4096)              296512    \n","                                                                 \n"," decoder (Sequential)        (None, 256, 256, 1)       21300480  \n","                                                                 \n"," classifier (Sequential)     (None, 3)                 4458499   \n","                                                                 \n","=================================================================\n","Total params: 26,055,491\n","Trainable params: 26,055,491\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["class MyModel(tf.keras.Model):\n","\n","  def __init__(self, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.augmentation = augmentation\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","  def call(self, inputs):\n","    x = self.augmentation(inputs)\n","    x = self.encoder(x)\n","    self.decoder(x)\n","    return self.classifier(x)\n","\n","classif_decoder = MyModel(name='classif_decoder')\n","\n","inputs = layers.Input(shape=input_shape)\n","classified_decoded = classif_decoder(inputs)\n","\n","model = keras.Model(inputs=inputs, outputs=classified_decoded)\n","model.get_layer('classif_decoder').summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHTBPjoxelbs"},"outputs":[],"source":["classif_decoder = model.get_layer('classif_decoder')\n","encoder_input = classif_decoder.get_layer('augmentation')\n","decoder_output = classif_decoder.get_layer('decoder')\n","\n","model_name = 'cnn'\n","cnn_history, cnn_conf_mat = run_experiment_autoencoder(\n","    model, encoder_input, decoder_output,\n","    ds_train, ds_valid, ds_test,\n","    batch_size=batch_size_cnn, num_epochs=num_epochs,\n","    learning_rate=learning_rate, weight_decay=weight_decay,\n","    lam_recon=lam_recon,\n","    from_logits=False, label_smoothing=label_smoothing,\n","    patience=patience, min_delta=min_delta,\n","    log_path=log_path, ckpt_path=ckpt_path,\n","    prefix=model_name\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ym52F7relbs"},"outputs":[],"source":["model_name = 'cnn'\n","checkpoint_filename = os.path.join(ckpt_path, model_name + 'weights.hdf5')\n","model.load_weights(checkpoint_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":38742,"status":"ok","timestamp":1674808188075,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"wqDBMbmOEJQo","outputId":"3416152b-a4df-4a51-fc41-ef45a02db1d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["213/213 [==============================] - 16s 53ms/step - loss: 0.4266 - accuracy: 0.9347\n","Test accuracy: 93.47%\n","213/213 [==============================] - 11s 49ms/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]},{"data":{"text/html":["\n","  <div id=\"df-49424432-64de-4e6d-8066-9c6b7ab2bc3a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>Real</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1975</td>\n","      <td>32</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>40</td>\n","      <td>2298</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>136</td>\n","      <td>46</td>\n","      <td>2071</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49424432-64de-4e6d-8066-9c6b7ab2bc3a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-49424432-64de-4e6d-8066-9c6b7ab2bc3a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-49424432-64de-4e6d-8066-9c6b7ab2bc3a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Predicted     0     1     2\n","Real                       \n","0          1975    32   133\n","1            40  2298    56\n","2           136    46  2071"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["y_test_pd, y_pred_pd, accuracy, conf_mat, report = compile_test_model(\n","    model,\n","    ds_test, batch_size_cnn,\n","    from_logits=False, label_smoothing=label_smoothing\n",")\n","\n","print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","print(report)\n","f_name = os.path.join(metric_path, model_name + '_report.txt')\n","with open(f_name, \"w\") as text_file:\n","  text_file.write(report)\n","\n","conf_mat\n","f_name = os.path.join(metric_path, model_name + '_conf_mat.joblib')\n","joblib.dump(conf_mat, f_name)\n"]},{"cell_type":"markdown","metadata":{"id":"VNuMMoQAiB7k"},"source":["Fine tuning cnn without decoder part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1288,"status":"ok","timestamp":1675001476937,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"oYjesDGREJQo","outputId":"cc5c22f9-7e49-4bb4-be58-39c2a88fc2bf"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]}],"source":["inputs = layers.Input(shape=input_shape)\n","augmented = augmentation(inputs)\n","encoded = encoder(augmented)\n","classified = classifier(encoded)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1675001481007,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"_0Qil9dRu8xi","outputId":"80172f69-2445-4397-aef9-918463581a61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 256, 256, 1)]     0         \n","                                                                 \n"," augmentation (Sequential)   (None, 256, 256, 1)       0         \n","                                                                 \n"," encoder (Sequential)        (None, 4096)              296512    \n","                                                                 \n"," classifier (Sequential)     (None, 3)                 4458499   \n","                                                                 \n","=================================================================\n","Total params: 4,755,011\n","Trainable params: 4,458,499\n","Non-trainable params: 296,512\n","_________________________________________________________________\n"]}],"source":["# Create the Keras model.\n","model_pure_cnn = keras.Model(inputs=inputs, outputs=classified)\n","model_pure_cnn.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1491124,"status":"ok","timestamp":1674810226542,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"T5ghH7W_f_1c","outputId":"dcb5d1f3-7f37-40b4-d30c-4d7f4792cd62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"name":"stdout","output_type":"stream","text":["679/679 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.8982\n","Epoch 1: val_accuracy improved from -inf to 0.91876, saving model to ../output/ckpt/pure_cnn_lr0.001_weights.hdf5\n","679/679 [==============================] - 252s 311ms/step - loss: 0.4813 - accuracy: 0.8982 - val_loss: 0.4737 - val_accuracy: 0.9188\n","Epoch 2/100\n","679/679 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.8971\n","Epoch 2: val_accuracy did not improve from 0.91876\n","679/679 [==============================] - 225s 302ms/step - loss: 0.5301 - accuracy: 0.8971 - val_loss: 0.5298 - val_accuracy: 0.9165\n","Epoch 3/100\n","679/679 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.8878\n","Epoch 3: val_accuracy did not improve from 0.91876\n","679/679 [==============================] - 223s 302ms/step - loss: 0.5960 - accuracy: 0.8878 - val_loss: 0.5948 - val_accuracy: 0.9141\n","Epoch 4/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6686 - accuracy: 0.8730\n","Epoch 4: val_accuracy did not improve from 0.91876\n","679/679 [==============================] - 226s 304ms/step - loss: 0.6686 - accuracy: 0.8730 - val_loss: 0.6626 - val_accuracy: 0.9092\n","Epoch 5/100\n","679/679 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.8548\n","Epoch 5: val_accuracy did not improve from 0.91876\n","679/679 [==============================] - 225s 305ms/step - loss: 0.7394 - accuracy: 0.8548 - val_loss: 0.7291 - val_accuracy: 0.8984\n","Epoch 6/100\n","679/679 [==============================] - ETA: 0s - loss: 0.8050 - accuracy: 0.8286\n","Epoch 6: val_accuracy did not improve from 0.91876\n","679/679 [==============================] - 222s 301ms/step - loss: 0.8050 - accuracy: 0.8286 - val_loss: 0.7921 - val_accuracy: 0.8768\n","213/213 [==============================] - 15s 62ms/step - loss: 0.4612 - accuracy: 0.9331\n","Test accuracy: 93.31%\n","213/213 [==============================] - 12s 50ms/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]}],"source":["model_name = 'pure_cnn_lr0.001'\n","pure_cnn_history, pure_cnn_conf_mat = run_experiment_pure_cnn(\n","    model_pure_cnn,\n","    ds_train, ds_valid, ds_test,\n","    batch_size=batch_size_cnn, num_epochs=num_epochs,\n","    learning_rate=learning_rate / 1000., weight_decay=weight_decay,\n","    from_logits=False, label_smoothing=label_smoothing,\n","    patience=patience, min_delta=min_delta_fine_tuning,\n","    log_path=log_path, ckpt_path=ckpt_path,\n","    prefix=model_name\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqubRojfEJQp"},"outputs":[],"source":["model_name = 'pure_cnn_lr0.001'\n","checkpoint_filename = os.path.join(ckpt_path, model_name + '_weights.hdf5')\n","model_pure_cnn.load_weights(checkpoint_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"elapsed":42903,"status":"ok","timestamp":1674810431924,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"8IU_UYRqEJQq","outputId":"089f59f9-eabd-4535-82cd-5a9c0c10153b"},"outputs":[{"name":"stdout","output_type":"stream","text":["213/213 [==============================] - 12s 51ms/step - loss: 0.4612 - accuracy: 0.9331\n","Test accuracy: 93.31%\n","213/213 [==============================] - 12s 50ms/step\n"]},{"data":{"text/html":["\n","  <div id=\"df-6d2723fc-49a3-407d-b25b-43953d972cca\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>Real</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1988</td>\n","      <td>21</td>\n","      <td>131</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>59</td>\n","      <td>2259</td>\n","      <td>76</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>129</td>\n","      <td>38</td>\n","      <td>2086</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d2723fc-49a3-407d-b25b-43953d972cca')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6d2723fc-49a3-407d-b25b-43953d972cca button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6d2723fc-49a3-407d-b25b-43953d972cca');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Predicted     0     1     2\n","Real                       \n","0          1988    21   131\n","1            59  2259    76\n","2           129    38  2086"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["y_test_pd, y_pred_pd, accuracy, conf_mat, report = compile_test_model(\n","    model_pure_cnn,\n","    ds_test, batch_size_cnn,\n","    from_logits=False, label_smoothing=label_smoothing\n",")\n","\n","print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","print(report)\n","f_name = os.path.join(metric_path, model_name + '_report.txt')\n","with open(f_name, \"w\") as text_file:\n","  text_file.write(report)\n","\n","conf_mat\n","f_name = os.path.join(metric_path, model_name + '_conf_mat.joblib')\n","joblib.dump(conf_mat, f_name)\n"]},{"cell_type":"markdown","metadata":{"id":"GjH7ZDF5iOyq"},"source":["Branch transformer on top of cnn."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1674810436407,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"-N61UqwQEJQq","outputId":"051cf74d-ea57-492e-b852-e1bb335451a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"encoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 256, 256, 128)     1280      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 128, 128, 128)    0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 128, 128, 128)     147584    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 64, 64, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 64, 64, 64)        73792     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 32, 32, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 32, 32, 64)        36928     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 4096)              0         \n","                                                                 \n","=================================================================\n","Total params: 296,512\n","Trainable params: 296,512\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["encoder.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":248,"status":"ok","timestamp":1675006146168,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"kX_B3Q95elbs"},"outputs":[],"source":["# Sub Model\n","shared_encoder = keras.Sequential(name='shared_encoder')\n","\n","for layer in encoder.layers[:-1]:\n","  shared_encoder.add(layer)\n","\n","for layer in shared_encoder.layers:\n","  layer.trainable = False\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5227,"status":"ok","timestamp":1675006155700,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"0qKrJS3yelbt","outputId":"25a09d91-707f-475d-c611-b1e49d9822ee"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"shared_encoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 256, 256, 128)     1280      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 128, 128, 128)    0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 128, 128, 128)     147584    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 64, 64, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 64, 64, 64)        73792     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 32, 32, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 32, 32, 64)        36928     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n"," 2D)                                                             \n","                                                                 \n","=================================================================\n","Total params: 296,512\n","Trainable params: 0\n","Non-trainable params: 296,512\n","_________________________________________________________________\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," augmentation (Sequential)      (None, 256, 256, 1)  0           ['input_1[0][0]']                \n","                                                                                                  \n"," shared_encoder (Sequential)    (None, 8, 8, 64)     296512      ['augmentation[0][0]']           \n","                                                                                                  \n"," patches (Patches)              (None, None, 64)     0           ['shared_encoder[0][0]']         \n","                                                                                                  \n"," patch_encoder (PatchEncoder)   (None, 64, 64)       8256        ['patches[0][0]']                \n","                                                                                                  \n"," layer_normalization (LayerNorm  (None, 64, 64)      128         ['patch_encoder[0][0]']          \n"," alization)                                                                                       \n","                                                                                                  \n"," multi_head_attention (MultiHea  (None, 64, 64)      132672      ['layer_normalization[0][0]',    \n"," dAttention)                                                      'layer_normalization[0][0]']    \n","                                                                                                  \n"," add (Add)                      (None, 64, 64)       0           ['multi_head_attention[0][0]',   \n","                                                                  'patch_encoder[0][0]']          \n","                                                                                                  \n"," layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['add[0][0]']                    \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_1 (Dense)                (None, 64, 128)      8320        ['layer_normalization_1[0][0]']  \n","                                                                                                  \n"," dropout (Dropout)              (None, 64, 128)      0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 64, 64)       8256        ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 64, 64)       0           ['dense_2[0][0]']                \n","                                                                                                  \n"," add_1 (Add)                    (None, 64, 64)       0           ['dropout_1[0][0]',              \n","                                                                  'add[0][0]']                    \n","                                                                                                  \n"," layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['add_1[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_1 (MultiH  (None, 64, 64)      132672      ['layer_normalization_2[0][0]',  \n"," eadAttention)                                                    'layer_normalization_2[0][0]']  \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64)       0           ['multi_head_attention_1[0][0]', \n","                                                                  'add_1[0][0]']                  \n","                                                                                                  \n"," layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_3 (Dense)                (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 64, 128)      0           ['dense_3[0][0]']                \n","                                                                                                  \n"," dense_4 (Dense)                (None, 64, 64)       8256        ['dropout_2[0][0]']              \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 64, 64)       0           ['dense_4[0][0]']                \n","                                                                                                  \n"," add_3 (Add)                    (None, 64, 64)       0           ['dropout_3[0][0]',              \n","                                                                  'add_2[0][0]']                  \n","                                                                                                  \n"," layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_2 (MultiH  (None, 64, 64)      132672      ['layer_normalization_4[0][0]',  \n"," eadAttention)                                                    'layer_normalization_4[0][0]']  \n","                                                                                                  \n"," add_4 (Add)                    (None, 64, 64)       0           ['multi_head_attention_2[0][0]', \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_5 (Dense)                (None, 64, 128)      8320        ['layer_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 64, 128)      0           ['dense_5[0][0]']                \n","                                                                                                  \n"," dense_6 (Dense)                (None, 64, 64)       8256        ['dropout_4[0][0]']              \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 64, 64)       0           ['dense_6[0][0]']                \n","                                                                                                  \n"," add_5 (Add)                    (None, 64, 64)       0           ['dropout_5[0][0]',              \n","                                                                  'add_4[0][0]']                  \n","                                                                                                  \n"," layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_3 (MultiH  (None, 64, 64)      132672      ['layer_normalization_6[0][0]',  \n"," eadAttention)                                                    'layer_normalization_6[0][0]']  \n","                                                                                                  \n"," add_6 (Add)                    (None, 64, 64)       0           ['multi_head_attention_3[0][0]', \n","                                                                  'add_5[0][0]']                  \n","                                                                                                  \n"," layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," dense_7 (Dense)                (None, 64, 128)      8320        ['layer_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 64, 128)      0           ['dense_7[0][0]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 64, 64)       8256        ['dropout_6[0][0]']              \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 64, 64)       0           ['dense_8[0][0]']                \n","                                                                                                  \n"," add_7 (Add)                    (None, 64, 64)       0           ['dropout_7[0][0]',              \n","                                                                  'add_6[0][0]']                  \n","                                                                                                  \n"," layer_normalization_8 (LayerNo  (None, 64, 64)      128         ['add_7[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," flatten_1 (Flatten)            (None, 4096)         0           ['layer_normalization_8[0][0]']  \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 4096)         0           ['flatten_1[0][0]']              \n","                                                                                                  \n"," dense_9 (Dense)                (None, 1024)         4195328     ['dropout_8[0][0]']              \n","                                                                                                  \n"," dropout_9 (Dropout)            (None, 1024)         0           ['dense_9[0][0]']                \n","                                                                                                  \n"," dense_10 (Dense)               (None, 256)          262400      ['dropout_9[0][0]']              \n","                                                                                                  \n"," dropout_10 (Dropout)           (None, 256)          0           ['dense_10[0][0]']               \n","                                                                                                  \n"," dense_11 (Dense)               (None, 3)            771         ['dropout_10[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,361,411\n","Trainable params: 5,064,899\n","Non-trainable params: 296,512\n","__________________________________________________________________________________________________\n"]}],"source":["\n","%aimport myLayers.vision_transformer\n","%aimport myLayers.mlp\n","from myLayers.vision_transformer import add_vit\n","from myLayers.mlp import mlp\n","\n","inputs = layers.Input(shape=input_shape)\n","augmented_transformer = augmentation(inputs)\n","shared_encoded = shared_encoder(augmented_transformer)\n","features = add_vit(shared_encoded,\n","            patch_size=patch_size,\n","            input_image_size=shared_encoded.shape[1],\n","            transformer_layers=transformer_layers,\n","            num_heads=num_heads,\n","            projection_dim=projection_dim,\n","            transformer_units_rate=transformer_units_rate,\n","            mlp_head_units=mlp_head_units)\n","# Classify outputs.\n","softmax = layers.Dense(3, activation='softmax', kernel_initializer='random_normal')(features)\n","\n","# Create the Keras model\n","model_transformer = keras.Model(inputs=inputs, outputs=softmax)\n","shared_encoder.summary()\n","model_transformer.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1981246,"status":"ok","timestamp":1674815639626,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"UdbqZ5jGelbt","outputId":"0585e73a-6425-4122-ec85-6d14ac8a30cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"name":"stdout","output_type":"stream","text":["679/679 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.6175\n","Epoch 1: val_accuracy improved from -inf to 0.68796, saving model to ../output/ckpt/transformer_8_8_lr0.5_weights.hdf5\n","679/679 [==============================] - 199s 247ms/step - loss: 0.9223 - accuracy: 0.6175 - val_loss: 0.8249 - val_accuracy: 0.6880\n","Epoch 2/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.7835\n","Epoch 2: val_accuracy improved from 0.68796 to 0.71104, saving model to ../output/ckpt/transformer_8_8_lr0.5_weights.hdf5\n","679/679 [==============================] - 182s 244ms/step - loss: 0.6964 - accuracy: 0.7835 - val_loss: 0.8053 - val_accuracy: 0.7110\n","Epoch 3/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.8029\n","Epoch 3: val_accuracy improved from 0.71104 to 0.71806, saving model to ../output/ckpt/transformer_8_8_lr0.5_weights.hdf5\n","679/679 [==============================] - 183s 246ms/step - loss: 0.6559 - accuracy: 0.8029 - val_loss: 0.8111 - val_accuracy: 0.7181\n","Epoch 4/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.8145\n","Epoch 4: val_accuracy improved from 0.71806 to 0.72120, saving model to ../output/ckpt/transformer_8_8_lr0.5_weights.hdf5\n","679/679 [==============================] - 181s 244ms/step - loss: 0.6356 - accuracy: 0.8145 - val_loss: 0.8649 - val_accuracy: 0.7212\n","Epoch 5/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.8236\n","Epoch 5: val_accuracy improved from 0.72120 to 0.74612, saving model to ../output/ckpt/transformer_8_8_lr0.5_weights.hdf5\n","679/679 [==============================] - 180s 242ms/step - loss: 0.6184 - accuracy: 0.8236 - val_loss: 0.7814 - val_accuracy: 0.7461\n","Epoch 6/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.8253\n","Epoch 6: val_accuracy did not improve from 0.74612\n","679/679 [==============================] - 180s 242ms/step - loss: 0.6111 - accuracy: 0.8253 - val_loss: 0.8098 - val_accuracy: 0.7112\n","Epoch 7/100\n","679/679 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.8291\n","Epoch 7: val_accuracy did not improve from 0.74612\n","679/679 [==============================] - 179s 241ms/step - loss: 0.6035 - accuracy: 0.8291 - val_loss: 0.7887 - val_accuracy: 0.7269\n","Epoch 8/100\n","679/679 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.8316\n","Epoch 8: val_accuracy did not improve from 0.74612\n","679/679 [==============================] - 181s 243ms/step - loss: 0.5960 - accuracy: 0.8316 - val_loss: 0.8035 - val_accuracy: 0.6992\n","Epoch 9/100\n","679/679 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.8369\n","Epoch 9: val_accuracy did not improve from 0.74612\n","679/679 [==============================] - 182s 240ms/step - loss: 0.5881 - accuracy: 0.8369 - val_loss: 0.9051 - val_accuracy: 0.6994\n","Epoch 10/100\n","679/679 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.8384\n","Epoch 10: val_accuracy did not improve from 0.74612\n","679/679 [==============================] - 183s 245ms/step - loss: 0.5836 - accuracy: 0.8384 - val_loss: 0.8731 - val_accuracy: 0.6928\n","213/213 [==============================] - 15s 66ms/step - loss: 0.7266 - accuracy: 0.7818\n","Test accuracy: 78.18%\n","213/213 [==============================] - 18s 66ms/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]}],"source":["%aimport run_exp.standard\n","from run_exp.standard import run_experiment as run_experiment_transformer\n","\n","model_name = 'transformer_8_8_lr0.5'\n","transformer_history, transformer_conf_mat = run_experiment_transformer(\n","    model_transformer,\n","    ds_train, ds_valid, ds_test,\n","    batch_size=batch_size_transformer, num_epochs=num_epochs,\n","    learning_rate=learning_rate / 2., weight_decay=weight_decay,\n","    from_logits=False, label_smoothing=label_smoothing,\n","    patience=patience, min_delta=min_delta,\n","    log_path=log_path, ckpt_path=ckpt_path,\n","    prefix=model_name\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crV7qWT834Vd"},"outputs":[],"source":["model_name = 'transformer_8_8_lr0.5_all_layers_lr'\n","checkpoint_filename = os.path.join(ckpt_path, model_name + '_weights.hdf5')\n","model_transformer.load_weights(checkpoint_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":66685,"status":"ok","timestamp":1674823216321,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"R1boKCYXEJQs","outputId":"09fe74b0-4f58-45aa-db20-de46d5aaf0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["213/213 [==============================] - 26s 72ms/step - loss: 0.4304 - accuracy: 0.9262\n","Test accuracy: 92.62%\n","213/213 [==============================] - 14s 59ms/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]},{"data":{"text/html":["\n","  <div id=\"df-10b39b90-d3a0-431a-9529-a739e9ec4845\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>Real</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2030</td>\n","      <td>13</td>\n","      <td>97</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>69</td>\n","      <td>2234</td>\n","      <td>91</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>213</td>\n","      <td>18</td>\n","      <td>2022</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10b39b90-d3a0-431a-9529-a739e9ec4845')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-10b39b90-d3a0-431a-9529-a739e9ec4845 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-10b39b90-d3a0-431a-9529-a739e9ec4845');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Predicted     0     1     2\n","Real                       \n","0          2030    13    97\n","1            69  2234    91\n","2           213    18  2022"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["y_test_pd, y_pred_pd, accuracy, conf_mat, report = compile_test_model(\n","    model_transformer,\n","    ds_test, batch_size_transformer,\n","    from_logits=False, label_smoothing=label_smoothing\n",")\n","\n","print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","print(report)\n","f_name = os.path.join(metric_path, model_name + '_report.txt')\n","with open(f_name, \"w\") as text_file:\n","  text_file.write(report)\n","\n","conf_mat\n","f_name = os.path.join(metric_path, model_name + '_conf_mat.joblib')\n","joblib.dump(conf_mat, f_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jFO6hmUGelA"},"outputs":[],"source":["for layer in shared_encoder.layers:\n","  layer.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7lG67hu4Akh"},"outputs":[],"source":["%aimport run_exp.standard\n","from run_exp.standard import run_experiment as run_experiment_transformer\n","\n","model_name = 'transformer_8_8_lr0.5_all_layers_lr_fine_tuning_lr0.0001'\n","transformer_fine_tuning_history, transformer_fine_tuning_conf_mat = run_experiment_transformer(\n","    model_transformer,\n","    ds_train, ds_valid, ds_test,\n","    batch_size=batch_size_transformer, num_epochs=num_epochs,\n","    learning_rate=learning_rate / 10000., weight_decay=weight_decay,\n","    from_logits=False, label_smoothing=label_smoothing,\n","    patience=patience, min_delta=min_delta_fine_tuning,\n","    log_path=log_path, ckpt_path=ckpt_path,\n","    prefix=model_name\n",")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1557,"status":"ok","timestamp":1675006257992,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"hhx43RoJEJQs"},"outputs":[],"source":["model_name = 'transformer_8_8_lr0.5_all_layers_lr_fine_tuning_lr0.0001'\n","checkpoint_filename = os.path.join(ckpt_path, model_name + '_weights.hdf5')\n","model_transformer.load_weights(checkpoint_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":66940,"status":"ok","timestamp":1675001166690,"user":{"displayName":"Luc Thomas","userId":"17860019511086541433"},"user_tz":-60},"id":"DPLMW69sEJQt","outputId":"7c9994a7-4aad-4a66-eac1-235c08d1359a"},"outputs":[{"name":"stdout","output_type":"stream","text":["213/213 [==============================] - 24s 63ms/step - loss: 0.4454 - accuracy: 0.9334\n","Test accuracy: 93.34%\n","213/213 [==============================] - 14s 59ms/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.89      0.92      2140\n","           1       0.98      0.96      0.97      2394\n","           2       0.88      0.95      0.91      2253\n","\n","    accuracy                           0.93      6787\n","   macro avg       0.93      0.93      0.93      6787\n","weighted avg       0.93      0.93      0.93      6787\n","\n"]},{"data":{"text/html":["\n","  <div id=\"df-9f79559c-66bf-47d0-ad2e-df8f5f1f6455\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","    <tr>\n","      <th>Real</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1907</td>\n","      <td>23</td>\n","      <td>210</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>32</td>\n","      <td>2294</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>87</td>\n","      <td>32</td>\n","      <td>2134</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f79559c-66bf-47d0-ad2e-df8f5f1f6455')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f79559c-66bf-47d0-ad2e-df8f5f1f6455 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f79559c-66bf-47d0-ad2e-df8f5f1f6455');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Predicted     0     1     2\n","Real                       \n","0          1907    23   210\n","1            32  2294    68\n","2            87    32  2134"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# test_accuracy(\n","#     model_transformer,\n","#     ds_train, batch_size_transformer,\n","# )\n","# test_accuracy(\n","#     model_transformer,\n","#     ds_valid, batch_size_transformer,\n","# )\n","# test_accuracy(\n","#     model_transformer,\n","#     ds_test, batch_size_transformer,\n","# )\n","\n","y_test_pd, y_pred_pd, accuracy, conf_mat, report = compile_test_model(\n","    model_transformer,\n","    ds_test, batch_size_transformer,\n","    from_logits=False, label_smoothing=label_smoothing\n",")\n","\n","print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","print(report)\n","f_name = os.path.join(metric_path, model_name + '_report.txt')\n","with open(f_name, \"w\") as text_file:\n","  text_file.write(report)\n","\n","conf_mat\n","f_name = os.path.join(metric_path, model_name + '_conf_mat.joblib')\n","joblib.dump(conf_mat, f_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ds_test_file_paths_pd = pd.DataFrame(data=ds_test_file_paths, columns=['file_path'])\n","ds_results_test = pd.concat([ds_test_file_paths_pd, y_test_pd, y_pred_pd])\n","ds_results_test.head()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"covid-19-xRay-gI8RPtYc","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ec449b28ee1275c8ed3472cdab9bc054b62d41bc2731e9c066fdcbfc125fb022"}}},"nbformat":4,"nbformat_minor":0}
