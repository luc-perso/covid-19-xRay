{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aomJUY_gvXed",
   "metadata": {
    "id": "aomJUY_gvXed"
   },
   "source": [
    "Initialise code for google colab\n",
    "\n",
    "Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GAjcLhjCgpxv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4450,
     "status": "ok",
     "timestamp": 1671446275823,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "GAjcLhjCgpxv",
    "outputId": "e34c2966-478f-439f-c4bb-ea2036820438"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_addons as tfa\n",
    "print(tfa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.11\n",
    "!pip install tensorflow-addons==0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J3iW3Uff2Jyh",
   "metadata": {
    "id": "J3iW3Uff2Jyh"
   },
   "source": [
    "Create data base files under google colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D1ubGnuD2CXZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88494,
     "status": "ok",
     "timestamp": 1671446369482,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "D1ubGnuD2CXZ",
    "outputId": "79fec16a-f91c-413d-f571-dd30d3435e5f"
   },
   "outputs": [],
   "source": [
    "!unzip -q '/content/drive/MyDrive/data_equalize.zip' -d '/content/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sMjhaRhC-HQM",
   "metadata": {
    "id": "sMjhaRhC-HQM"
   },
   "source": [
    "For Luc because my archive made on mac create a __MACOSX folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iAyxXHqB935f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2940,
     "status": "ok",
     "timestamp": 1671446392319,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "iAyxXHqB935f"
   },
   "outputs": [],
   "source": [
    "%rm -rf /content/__MACOSX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "nnQuhv17x7hK",
   "metadata": {
    "id": "nnQuhv17x7hK"
   },
   "source": [
    "Define working directory to our jupyter repertory:\n",
    "* because path to the different repertories (./data, ./output...) are define relatevly to jupyter one\n",
    "* let import _mypath which add ./lib to python path in order to import our own define libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9WqQG1rjjHGq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1671446474973,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "9WqQG1rjjHGq",
    "outputId": "89a23162-4e46-4cd1-f96d-5c67bb7996aa"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/covid-19-xRay/jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for google colab use\n",
    "db_work_dir = '/content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local use\n",
    "db_work_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c135d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _mypath\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.path_origin_data import build_data_paths \n",
    "from database.path_origin_data import lung_name, infection_name\n",
    "from database.path_origin_data import train_name, test_name, valid_name\n",
    "from database.path_origin_data import normal_name, covid_name, no_covid_name\n",
    "from database.path_origin_data import images_name, lung_mask_name, infection_mask_name\n",
    "\n",
    "from database.dataset import build_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76855cab",
   "metadata": {
    "id": "76855cab"
   },
   "source": [
    "Build paths and variables for reading data base hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'data_equalize'\n",
    "db_path = os.path.join(db_work_dir, db_name)\n",
    "\n",
    "output_path = os.path.join('..', 'output')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec096c7",
   "metadata": {
    "id": "fec096c7"
   },
   "source": [
    "Structure to manage paths in data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1271f47",
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1671446527378,
     "user": {
      "displayName": "Luc Thomas",
      "userId": "17860019511086541433"
     },
     "user_tz": -60
    },
    "id": "a1271f47"
   },
   "outputs": [],
   "source": [
    "data_paths = build_data_paths()\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = data_paths['path']\n",
    "\n",
    "ds_train = build_dataset(db_path, paths, db=[lung_name], ds=[train_name])\n",
    "ds_test = build_dataset(db_path, paths, db=[lung_name], ds=[test_name])\n",
    "ds_valid = build_dataset(db_path, paths, db=[lung_name], ds=[valid_name])\n",
    "ds_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 256\n",
    "input_shape = (image_size, image_size, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "label_smoothing = 0.01\n",
    "lam_recon = 10.\n",
    "\n",
    "# data augmentation\n",
    "scale = 1. / 255.\n",
    "flip = \"horizontal\"\n",
    "rotation_factor = 0.1\n",
    "zoom_height_factor = 0.2\n",
    "zoom_width_factor = 0.2\n",
    "\n",
    "# vit\n",
    "patch_size = 1\n",
    "transformer_layers = 2\n",
    "num_heads = 4\n",
    "projection_dim = 64\n",
    "transformer_units_rate = [2, 1]\n",
    "mlp_head_units = [512, 256]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = keras.Sequential(\n",
    "    [\n",
    "      layers.Rescaling(scale=scale),\n",
    "      layers.RandomFlip(flip),\n",
    "      layers.RandomRotation(rotation_factor),\n",
    "      layers.RandomZoom(height_factor=zoom_height_factor, width_factor=zoom_width_factor),\n",
    "    ],\n",
    "    name='augmentation'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the CNN\n",
    "# encoder = keras.Sequential(\n",
    "    [\n",
    "      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(32, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(32, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Flatten(),\n",
    "    ],\n",
    "    name='encoder'\n",
    ")\n",
    "\n",
    "encoder = keras.Sequential(\n",
    "    [\n",
    "      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(128, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "      layers.Conv2D(64, (3, 3), activation = 'relu', padding='same', kernel_initializer='random_normal'),\n",
    "      layers.Flatten(),\n",
    "    ],\n",
    "    name='encoder'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = keras.Sequential(\n",
    "    [\n",
    "      layers.Dense(512, activation='relu'),\n",
    "      layers.Dense(np.prod(input_shape), activation='sigmoid'),\n",
    "      layers.Reshape(target_shape=input_shape),\n",
    "    ],\n",
    "    name='decoder'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential(\n",
    "    [\n",
    "      layers.Dense(512, activation='relu'),\n",
    "      layers.Dense(256, activation='relu'),\n",
    "      layers.Dense(3, activation='softmax'),\n",
    "    ],\n",
    "    name='classifier'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.augmentation = augmentation\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.classifier = classifier\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.augmentation(inputs)\n",
    "    x = self.encoder(x)\n",
    "    self.decoder(x)\n",
    "    return self.classifier(x)\n",
    "\n",
    "classif_decoder = MyModel(name='classif_decoder')\n",
    "classified = classif_decoder(inputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=classified)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('classif_decoder').summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport run_exp.classif_autoencoder\n",
    "from run_exp.classif_autoencoder import run_experiment as run_experiment_autoencoder\n",
    "\n",
    "classif_decoder = model.get_layer('classif_decoder')\n",
    "encoder_input = classif_decoder.get_layer('augmentation')\n",
    "decoder_output = classif_decoder.get_layer('decoder')\n",
    "\n",
    "cnn_history, cnn_conf_mat = run_experiment_autoencoder(model, encoder_input, decoder_output,\n",
    "              ds_train, ds_test, ds_valid,\n",
    "              batch_size=batch_size, num_epochs=num_epochs,\n",
    "              learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "              lam_recon=lam_recon,\n",
    "              output_path=output_path, prefix='cnn',\n",
    "              from_logits=False, label_smoothing=label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "file_name = os.path.join(output_path, 'cnn_conf_mat.joblib')\n",
    "joblib.dump(cnn_conf_mat, file_name)\n",
    "\n",
    "cnn_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(output_path, 'ckpt')\n",
    "checkpoint_filename = os.path.join(ckpt_path, 'cnn_weights.hdf5')\n",
    "model.load_weights(checkpoint_filename)\n",
    "\n",
    "# model.get_layer('classif_decoder').summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = model.get_layer('classif_decoder').get_layer('encoder')\n",
    "# encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_names = [layer.name for layer in encoder.layers]\n",
    "# layer_idx = layer_names.index('max_pooling2d_8')\n",
    "\n",
    "# Sub Model\n",
    "shared_encoder = tf.keras.Sequential(name='shared_encoder')\n",
    "\n",
    "for layer in encoder.layers[:-1]:\n",
    "  shared_encoder.add(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%aimport myLayers.vision_transformer\n",
    "%aimport myLayers.mlp\n",
    "from myLayers.vision_transformer import add_vit\n",
    "from myLayers.mlp import mlp\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "augmented_2 = augmentation(inputs)\n",
    "shared_model = shared_encoder(augmented_2)\n",
    "features = add_vit(shared_model,\n",
    "            patch_size=patch_size,\n",
    "            input_image_size=shared_model.shape[1],\n",
    "            transformer_layers=transformer_layers,\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            transformer_units_rate=transformer_units_rate,\n",
    "            mlp_head_units=mlp_head_units)\n",
    "# Classify outputs.\n",
    "softmax = layers.Dense(3, activation='softmax', kernel_initializer='random_normal')(features)\n",
    "\n",
    "# Create the Keras model.\n",
    "model_transformer = keras.Model(inputs=inputs, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport run_exp.standard\n",
    "from run_exp.standard import run_experiment as run_experiment_transformer\n",
    "\n",
    "transformer_history, transformer_conf_mat = run_experiment_transformer(model,\n",
    "              ds_train, ds_test, ds_valid,\n",
    "              batch_size=batch_size, num_epochs=num_epochs,\n",
    "              learning_rate=learning_rate, weight_decay=weight_decay,\n",
    "              output_path=output_path, prefix='transformer',\n",
    "              from_logits=False, label_smoothing=label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "file_name = os.path.join(output_path, 'transformer_conf_mat.joblib')\n",
    "joblib.dump(transformer_conf_mat, file_name)\n",
    "\n",
    "transformer_conf_mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid-19-xRay-gI8RPtYc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec449b28ee1275c8ed3472cdab9bc054b62d41bc2731e9c066fdcbfc125fb022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
